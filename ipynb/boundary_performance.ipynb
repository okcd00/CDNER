{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "economic-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/chendian/pure')\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import jsonlines\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "careful-incident",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "    @classmethod\n",
    "    def _read_text(self,input_file):\n",
    "        lines = []\n",
    "        with open(input_file,'r') as f:\n",
    "            words = []\n",
    "            labels = []\n",
    "            for line in f:\n",
    "                if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                    if words:\n",
    "                        lines.append({\"words\":words,\"labels\":labels})\n",
    "                        words = []\n",
    "                        labels = []\n",
    "                else:\n",
    "                    # splits = line.split(\" \")\n",
    "                    splits = line.split()\n",
    "                    words.append(splits[0])\n",
    "                    if len(splits) > 1:\n",
    "                        labels.append(splits[-1].replace(\"\\n\", \"\"))\n",
    "                    else:\n",
    "                        # Examples could have no label for mode = \"test\"\n",
    "                        labels.append(\"O\")\n",
    "            if words:\n",
    "                lines.append({\"words\":words,\"labels\":labels})\n",
    "        return lines\n",
    "\n",
    "    @classmethod\n",
    "    def _read_json(self,input_file):\n",
    "        lines = []\n",
    "        with open(input_file,'r') as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line.strip())\n",
    "                text = line['text']\n",
    "                label_entities = line.get('label',None)\n",
    "                words = list(text)\n",
    "                labels = ['O'] * len(words)\n",
    "                if label_entities is not None:\n",
    "                    for key,value in label_entities.items():\n",
    "                        for sub_name,sub_index in value.items():\n",
    "                            for start_index,end_index in sub_index:\n",
    "                                assert  ''.join(words[start_index:end_index+1]) == sub_name\n",
    "                                if start_index == end_index:\n",
    "                                    labels[start_index] = 'S-'+key\n",
    "                                else:\n",
    "                                    labels[start_index] = 'B-'+key\n",
    "                                    labels[start_index+1:end_index+1] = ['I-'+key]*(len(sub_name)-1)\n",
    "                lines.append({\"words\": words, \"labels\": labels})\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caring-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DataProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_bios(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIOS\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        # >>> seq = ['B-PER', 'I-PER', 'O', 'S-LOC']\n",
    "        # >>> get_entity_bios(seq)\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if tag.startswith(\"S-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[2] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "            chunks.append(chunk)\n",
    "            chunk = (-1, -1, -1)\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "        elif tag.startswith('I-') and chunk[1] != -1:\n",
    "            _type = tag.split('-')[1]\n",
    "            if _type == chunk[0]:\n",
    "                chunk[2] = indx\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return chunks\n",
    "\n",
    "def get_entity_bio(seq):\n",
    "    \"\"\"Gets entities from sequence.\n",
    "    note: BIO\n",
    "    Args:\n",
    "        seq (list): sequence of labels.\n",
    "    Returns:\n",
    "        list: list of (chunk_type, chunk_start, chunk_end).\n",
    "    Example:\n",
    "        seq = ['B-PER', 'I-PER', 'O', 'B-LOC']\n",
    "        get_entity_bio(seq)\n",
    "        #output\n",
    "        [['PER', 0,1], ['LOC', 3, 3]]\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    chunk = [-1, -1, -1]\n",
    "    for indx, tag in enumerate(seq):\n",
    "        if tag.startswith(\"B-\"):\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "            chunk[1] = indx\n",
    "            chunk[0] = tag.split('-')[1]\n",
    "            chunk[2] = indx\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        elif tag.startswith('I-') and chunk[1] != -1:\n",
    "            _type = tag.split('-')[1]\n",
    "            if _type == chunk[0]:\n",
    "                chunk[2] = indx\n",
    "\n",
    "            if indx == len(seq) - 1:\n",
    "                chunks.append(chunk)\n",
    "        else:\n",
    "            if chunk[2] != -1:\n",
    "                chunks.append(chunk)\n",
    "            chunk = [-1, -1, -1]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ruled-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\n",
      "drwxrwxr-x 2 chendian chendian 4.0K May  7 05:25 msra\n",
      "drwxrwxr-x 2 chendian chendian 4.0K May  6 20:27 onto4\n",
      "drwxrwxr-x 2 chendian chendian 4.0K May  6 20:11 resume\n",
      "drwxrwxr-x 2 chendian chendian 4.0K May  6 15:56 cner\n",
      "drwxrwxr-x 2 chendian chendian 4.0K May  6 15:56 cluener\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /home/chendian/BERT-NER-Pytorch/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "furnished-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40K\n",
      "-rw-rw-r-- 1 chendian chendian 7.3K May  6 20:31 bert-onto4-2022-05-06-20:30:58.log\n",
      "-rw-rw-r-- 1 chendian chendian  29K May  6 20:27 bert-onto4-2022-05-06-20:27:13.log\n"
     ]
    }
   ],
   "source": [
    "!ls -lht /data/chendian/bert_ner_output/onto4_output/macbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-vietnam",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "is_tagger_model = False\n",
    "boundary_error_examples = []\n",
    "\n",
    "for task_name in ['msra', 'resume', 'onto4']:\n",
    "    for bert_type in ['bert', 'macbert']:\n",
    "        tagger_str = '_tagger' if is_tagger_model else ''\n",
    "        \n",
    "        print(f\"{bert_type}{tagger_str} model on {task_name} dataset: \")\n",
    "        \n",
    "        if task_name == 'msra':\n",
    "            ge_func = get_entity_bio\n",
    "        else:\n",
    "            ge_func = get_entity_bios\n",
    "\n",
    "        test_answer_path = f'/home/chendian/BERT-NER-Pytorch/datasets/{task_name}/test.ner'\n",
    "        test_answers = dp._read_text(test_answer_path)  # a list of {'words': [], 'labels': []}\n",
    "\n",
    "        test_log_path = f'/data/chendian/bert_ner_output/{task_name}_{bert_type}{tagger_str}_output/bert/test_results.txt'\n",
    "        test_json_path = f'/data/chendian/bert_ner_output/{task_name}_{bert_type}{tagger_str}_output/bert/test_prediction.json'\n",
    "        \n",
    "        model_logs = [line.strip() for line in open(test_log_path, 'r')]\n",
    "        lines = [line.strip() for line in open(test_json_path, 'r')]\n",
    "        # print(len(lines), len(test_answers))\n",
    "\n",
    "\n",
    "        hit = 0\n",
    "        total = 0\n",
    "        others = 0\n",
    "        err_type = 0\n",
    "        mis_report = 0\n",
    "        err_boundary = 0\n",
    "\n",
    "        for line_id, line in tqdm(enumerate(lines)):\n",
    "            res = json.loads(line)  # prediction results\n",
    "            pred = res['entities']\n",
    "            answers = test_answers[line_id+1]\n",
    "            truth_labels = [t.replace('E-', 'I-').replace('M-', 'I-') for t in answers['labels']][:510]\n",
    "            try:\n",
    "                assert len(res['tag_seq'].split()) == len(truth_labels)\n",
    "            except:\n",
    "                print(\">\", line_id)\n",
    "                print(len(res['tag_seq'].split()), len(truth_labels))\n",
    "                print(res['tag_seq'].split())\n",
    "                print(truth_labels)\n",
    "            truth = ge_func(truth_labels)\n",
    "            # print(line_id, pred, truth)\n",
    "            # print(res['tag_seq'].split())\n",
    "            # print(truth_labels)\n",
    "            ner_offset = [0 for _ in range(len(truth_labels))]\n",
    "            span2tag = {(l, r): t for t, l, r in truth}\n",
    "            for truth_ner in truth:\n",
    "                t, l, r = truth_ner\n",
    "                l, r = int(l), int(r)\n",
    "                for idx in range(l, r+1):\n",
    "                    ner_offset[idx] = 1\n",
    "            for predict_ner in pred:\n",
    "                total += 1\n",
    "                t, l, r = predict_ner\n",
    "                if predict_ner in truth:\n",
    "                    hit += 1\n",
    "                elif span2tag.get((l, r)) is not None:\n",
    "                    err_type += 1\n",
    "                elif sum([ner_offset[_i] for _i in range(l, r+1)]) >= 1:\n",
    "                    err_boundary += 1\n",
    "                    if bert_type == 'macbert':\n",
    "                        boundary_error_examples.append({\n",
    "                            'sample_id': line_id,\n",
    "                            'task_name': task_name,\n",
    "                            'predict': predict_ner,\n",
    "                            'truth': truth,\n",
    "                            'sentence': ''.join(answers['words']),\n",
    "                            'predict_entity': [''.join(answers['words'][predict_ner[1]:predict_ner[2]+1])]\n",
    "                        })\n",
    "                        # pprint(boundary_error_examples[-1])\n",
    "                        # raise ValueError()\n",
    "                elif sum([ner_offset[_i] for _i in range(l, r+1)]) == 0:\n",
    "                    mis_report += 1\n",
    "                else:\n",
    "                    others += 1\n",
    "\n",
    "        print(\"\t\".join(map(str, [total, hit, err_type, err_boundary, mis_report, others])))\n",
    "        print(model_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-token",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_answers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "raising-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2item = {item['sentence']: item for item in boundary_error_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "retired-negative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/chendian/pure_output_dir/msra_origin_macB_prop10_bs8_220505\n",
      "/data/chendian/pure_output_dir/resume_macB_prop10_bs8_220505\n",
      "/data/chendian/pure_output_dir/onto4_macB_prop10_bs8_220505\n"
     ]
    }
   ],
   "source": [
    "for model_path in ['msra_origin_macB_prop10_bs8_220505', 'resume_macB_prop10_bs8_220505', 'onto4_macB_prop10_bs8_220505']:\n",
    "    prediction_file = '/data/chendian/pure_output_dir/' + model_path + \n",
    "    print(prediction_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "banner-charleston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4346 samples from 1 documents, with 7684 NER labels, 47.875 avg input length, 271 max length\n",
      "Max Length: 271, max NER: 32\n",
      "Span Candidates' Count: 1858719, Cover: 7684\n",
      "Evaluating...\n",
      "Accuracy: 0.998594\n",
      "Cor: 6159, Pred TOT: 7403, Gold TOT: 7684\n",
      "P: 0.83196, R: 0.80154, F1: 0.81646\n",
      "Used time: 107.451279\n",
      "Total pred entities: 7403\n",
      "Output predictions to /data/chendian/pure_output_dir/onto4_macB_prop10_bs8_220505/ent_pred_test.json..\n"
     ]
    }
   ],
   "source": [
    "!tail /data/chendian/pure_output_dir/onto4_macB_prop10_bs8_220505/train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "complete-israeli",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MacBert-Prop model on msra dataset: \n",
      "6153\t5931\t15\t120\t87\t0\n",
      "MacBert-Prop model on resume dataset: \n",
      "1633\t1577\t1\t46\t9\t0\n",
      "MacBert-Prop model on onto4 dataset: \n",
      "7400\t6141\t162\t411\t686\t0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "for model_path in [\n",
    "    'msra_origin_macB_times20_220406', \n",
    "    'resume_macB_times20_220430', \n",
    "    'onto4_macB_times20_220430'\n",
    "    'weibo_macB_ratio20_221003']:\n",
    "# for model_path in ['msra_origin_macB_prop10_bs8_220505', 'resume_macB_prop10_bs8_220505', 'onto4_macB_prop10_bs8_220505']:\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    others = 0\n",
    "    err_type = 0\n",
    "    mis_report = 0\n",
    "    err_boundary = 0\n",
    "\n",
    "    prediction_file = '/data/chendian/pure_output_dir/' + model_path + '/ent_pred_test.json'\n",
    "    ret = json.load(open(prediction_file, 'r'))\n",
    "    # print(ret.keys())\n",
    "    sentences = ret['sentences']\n",
    "    ner = ret['ner']\n",
    "    sentences = ret['sentences']\n",
    "    predicted_ner = ret['predicted_ner']\n",
    "    offset = 0\n",
    "    for pred, truth, sent in zip(predicted_ner + [''] * 5, ner, sentences):\n",
    "        # print(pred)\n",
    "        # print(truth)\n",
    "        # print(sent)\n",
    "        text = ''.join(sent)\n",
    "        if text in sent2item:\n",
    "            print(sent2item[text]['task_name'], '-', sent2item[text]['sample_id'])\n",
    "            print(text)\n",
    "            _t, _l, _r = sent2item[text]['predict']  # err_span\n",
    "            truth_no_off = sent2item[text]['truth']\n",
    "            crf_prediction = sent2item[text]['predict'] + sent2item[text]['predict_entity']\n",
    "            print(\"MacBERT-CRF:\", crf_prediction)\n",
    "            for _a, _b, _c in zip(truth_no_off, pred, truth):\n",
    "                if set(range(int(_l), int(_r)+1)).intersection(set(range(int(_a[1]), int(_a[2])+1))):\n",
    "                    _b = [_b[2], int(_b[0])-offset, int(_b[1])-offset]\n",
    "                    _b.append(''.join([sent[_ci] for _ci in range(_b[1], _b[2]+1)]))\n",
    "                    _c = [_c[2], int(_c[0])-offset, int(_c[1])-offset]\n",
    "                    _c.append(''.join([sent[_ci] for _ci in range(_c[1], _c[2]+1)]))\n",
    "                    if _b[1:] == crf_prediction[1:]:\n",
    "                        del sent2item[text]\n",
    "                        break\n",
    "                    print(\"Ours:\", _b)\n",
    "                    print(\"Truth:\", _c)\n",
    "            print(\"解释：\")\n",
    "            print(pred)\n",
    "            # print(truth)\n",
    "            raise ValueError()\n",
    "        ner_offset = defaultdict(int)\n",
    "        span2tag = {(l, r): t for l, r, t in truth}\n",
    "        for truth_ner in truth:\n",
    "            l, r, t = truth_ner\n",
    "            l, r = int(l), int(r)\n",
    "            for idx in range(l, r+1):\n",
    "                ner_offset[idx] = 1\n",
    "        for predict_ner in pred:\n",
    "            total += 1\n",
    "            l, r, t = predict_ner\n",
    "            if predict_ner in truth:\n",
    "                hit += 1\n",
    "            elif span2tag.get((l, r)) is not None:\n",
    "                err_type += 1\n",
    "            elif sum([ner_offset[_i] for _i in range(l, r+1)]) >= 1:\n",
    "                err_boundary += 1\n",
    "            elif sum([ner_offset[_i] for _i in range(l, r+1)]) == 0:\n",
    "                mis_report += 1\n",
    "            else:\n",
    "                others += 1\n",
    "        offset += len(sent)\n",
    "\n",
    "    # print(f\"MacBert-Ratio20 model on {model_path.split('_')[0]} dataset: \")\n",
    "    print(f\"MacBert-Prop model on {model_path.split('_')[0]} dataset: \")\n",
    "    print(\"\t\".join(map(str, [total, hit, err_type, err_boundary, mis_report, others])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "endangered-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span-based Model on scierc dataset: \n",
      "total\thit\terr_type\t\t\terr_boundary\t\t\tmis_report\tothers\n",
      "1660\t1114\t210\t0.38461538461538464\t178\t0.326007326007326\t158\t0.2893772893772894\t0\n",
      "Span-based Model on ace04 dataset: \n",
      "total\thit\terr_type\t\t\terr_boundary\t\t\tmis_report\tothers\n",
      "2952\t2542\t157\t0.3829268292682927\t181\t0.44146341463414634\t72\t0.17560975609756097\t0\n",
      "Span-based Model on ace05 dataset: \n",
      "total\thit\terr_type\t\t\terr_boundary\t\t\tmis_report\tothers\n",
      "2957\t2537\t152\t0.3619047619047619\t145\t0.34523809523809523\t123\t0.29285714285714287\t0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "for model_path in ['scierc_220612', 'ace04_220612', 'ace05_220612']:\n",
    "    hit = 0\n",
    "    total = 0\n",
    "    others = 0\n",
    "    err_type = 0\n",
    "    mis_report = 0\n",
    "    err_boundary = 0\n",
    "\n",
    "    prediction_file = '/data/chendian/pure_output_dir/' + model_path + '/ent_pred_test.json'\n",
    "    for line in open(prediction_file, 'r'):\n",
    "        ret = json.loads(line)\n",
    "        ner = ret['ner']\n",
    "        sentences = ret['sentences']\n",
    "        predicted_ner = ret['predicted_ner']\n",
    "        for pred, truth, sent in zip(predicted_ner, ner, sentences):\n",
    "            # print(pred)\n",
    "            # print(truth)\n",
    "            # print(sent)\n",
    "            ner_offset = defaultdict(int)\n",
    "            span2tag = {(l, r): t for l, r, t in truth}\n",
    "            for truth_ner in truth:\n",
    "                l, r, t = truth_ner\n",
    "                l, r = int(l), int(r)\n",
    "                for idx in range(l, r+1):\n",
    "                    ner_offset[idx] = 1\n",
    "            for predict_ner in pred:\n",
    "                total += 1\n",
    "                l, r, t = predict_ner\n",
    "                if predict_ner in truth:\n",
    "                    hit += 1\n",
    "                elif span2tag.get((l, r)) is not None:\n",
    "                    err_type += 1\n",
    "                elif sum([ner_offset[_i] for _i in range(l, r+1)]) >= 1:\n",
    "                    err_boundary += 1\n",
    "                elif sum([ner_offset[_i] for _i in range(l, r+1)]) == 0:\n",
    "                    mis_report += 1\n",
    "                else:\n",
    "                    others += 1\n",
    "    print(f\"Span-based Model on {model_path.split('_')[0]} dataset: \")\n",
    "    print('\\t'.join(\"total, hit, err_type, , , err_boundary, , , mis_report, others\".split(', ')))\n",
    "    errs = total - hit\n",
    "    print(\"\t\".join(map(str, [total, hit, err_type, err_type/errs, err_boundary, err_boundary/errs, mis_report, mis_report/errs, others])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-titanium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "miniature-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sent2item[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "optical-dutch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请问这位秘书长是：（１）萨利姆（２）穆加贝（３）埃西５．６月６日，孟加拉国、印度、斯里兰卡、泰国四国签署宣言，宣布成立孟、印、斯、泰经济合作组织。'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "going-airline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mc21\u001b[m  Tue May 10 01:14:03 2022\n",
      "\u001b[36m[0]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m11178\u001b[m MB |\n",
      "\u001b[36m[1]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 33'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m11178\u001b[m MB |\n",
      "\u001b[36m[2]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 27'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m11178\u001b[m MB |\n",
      "\u001b[36m[3]\u001b[m \u001b[34mNVIDIA GeForce GTX 1080 Ti\u001b[m |\u001b[31m 29'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m11178\u001b[m MB |\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pure",
   "language": "python",
   "name": "pure"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
