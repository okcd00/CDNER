{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "running-sweden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/home/chendian/CDNER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proved-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_dir = '/home/chendian/CDNER/data'\n",
    "en_dataset_names = [\n",
    "    'conll03', 'ace04', 'ace05', 'scierc',\n",
    "]\n",
    "\n",
    "cn_dataset_names = [\n",
    "    'msra_origin', 'resume', 'onto4', 'onto5', # 'cluener'\n",
    "]\n",
    "\n",
    "entity_count = {\n",
    "    'msra_origin': 75059, # 45033\n",
    "    'resume': 13438,  # in 3821\n",
    "    'onto4': 13372,  # in 15724\n",
    "    'onto5': 0,  #\n",
    "    'weibo': 0,  #\n",
    "    'cluener': 23338,  # in 10748\n",
    "    'findoc': 120685,  # in 99581\n",
    "    'scierc': 5598,  # in 1861\n",
    "    'conll03': 23497,  # in 14040\n",
    "    'ace04': 22204,  # in 6202\n",
    "    'ace05': 24827,  # in 7299\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "future-arnold",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'msra_origin': 367.731108298359,\n",
      "  'onto4': 201.8200839481048,\n",
      "  'onto5': 210.62296160276262,\n",
      "  'resume': 254.62182674692488},\n",
      " {'msra_origin': 48.22054937490285,\n",
      "  'onto4': 31.283579241923174,\n",
      "  'onto5': 32.81938772713569,\n",
      "  'resume': 32.47814708191573},\n",
      " {'msra_origin': 15.155198277408935,\n",
      "  'onto4': 12.239122434536448,\n",
      "  'onto5': 13.979538546559823,\n",
      "  'resume': 14.894167392115408},\n",
      " {'msra_origin': 45033, 'onto4': 15724, 'onto5': 36487, 'resume': 3821}]\n",
      "344.93199353205046 48.26722114195915 18.756008883540204\n",
      "PHI: msra_origin 219.62690683329117\n",
      "PHI: resume 71.39991070099717\n",
      "PHI: onto4 236.3182022135806\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'onto5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6a2b621bd574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_spans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_chars_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_spans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PHI:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_spans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mentity_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'onto5'"
     ]
    }
   ],
   "source": [
    "n_samples = {}\n",
    "n_spans = {}\n",
    "n_chars = {}\n",
    "n_chars_seg = {}\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "for dn in cn_dataset_names:\n",
    "    file_path = f\"{data_dir}/{dn}/train.json\"\n",
    "    if dn == 'scierc':\n",
    "        n_entities = 0\n",
    "        sentences = []\n",
    "        for line in open(file_path, 'r'):\n",
    "            sentences.extend(json.loads(line)['sentences'])\n",
    "            n_entities += len(json.loads(line)['ner'])\n",
    "        print(n_entities)\n",
    "    else:\n",
    "        doc = json.load(open(file_path, 'r'))\n",
    "        sentences = doc['sentences']\n",
    "        n_entities = len(doc['ner'])\n",
    "        print(n_entities)\n",
    "    \n",
    "    n_samples[dn] = len(sentences)\n",
    "    spans = []\n",
    "    seg_length = []\n",
    "    for sent in sentences:\n",
    "        _n_span = 0\n",
    "        streak = 0\n",
    "        for c in sent:\n",
    "            # if c in ['，']:\n",
    "            if c in ['，', '；', '。', '？', '！']:\n",
    "                if streak > 25:\n",
    "                    _n_span += (streak - 25) * 25\n",
    "                else:\n",
    "                    _n_span += (1 + streak) * streak / 2\n",
    "                seg_length.append(streak)\n",
    "                streak = 0\n",
    "            streak += 1\n",
    "        spans.append(_n_span)\n",
    "    n_spans[dn] = sum(spans) / len(spans)\n",
    "    n_chars[dn] = sum(map(len, sentences)) / len(sentences)\n",
    "    n_chars_seg[dn] = sum(seg_length) / len(seg_length)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint([n_spans, n_chars, n_chars_seg, n_samples])\n",
    "print(sum(n_spans.values())/3, sum(n_chars.values())/3, sum(n_chars_seg.values())/3)\n",
    "for dn in n_spans:\n",
    "    print('PHI:', dn, n_spans[dn] * n_samples[dn] / entity_count[dn] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "front-barbados",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'findoc': 608.905172673502},\n",
      " {'findoc': 75.82712565650074},\n",
      " {'findoc': 21.1493599839996},\n",
      " {'findoc': 99581}]\n",
      "608.905172673502 75.82712565650074 21.1493599839996\n",
      "PHI: 501.426863321871\n"
     ]
    }
   ],
   "source": [
    "n_samples = {}\n",
    "n_spans = {}\n",
    "n_chars = {}\n",
    "n_chars_seg = {}\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "for dn in ['findoc']:\n",
    "    file_path = f\"{data_dir}/{dn}/train.json\"\n",
    "    if dn == 'findoc':\n",
    "        sentences = []\n",
    "        for line in open(file_path, 'r'):\n",
    "            sentences.extend(json.loads(line)['sentences'])\n",
    "    else:\n",
    "        sentences = json.load(open(file_path, 'r'))['sentences']\n",
    "    \n",
    "    n_samples[dn] = len(sentences)\n",
    "    spans = []\n",
    "    seg_length = []\n",
    "    for sent in sentences:\n",
    "        _n_span = 0\n",
    "        streak = 0\n",
    "        for c in sent:\n",
    "            # if c in ['，']:\n",
    "            if c in ['，', '；', '。', '？', '！']:\n",
    "                if streak > 25:\n",
    "                    _n_span += (streak - 25) * 25\n",
    "                else:\n",
    "                    _n_span += (1 + streak) * streak / 2\n",
    "                seg_length.append(streak)\n",
    "                streak = 0\n",
    "            else:\n",
    "                streak += 1\n",
    "        else:\n",
    "            if streak > 25:\n",
    "                _n_span += (streak-25) * 25\n",
    "            else:\n",
    "                _n_span += (1 + streak) * streak / 2 \n",
    "            seg_length.append(streak)\n",
    "            streak = 0\n",
    "        spans.append(_n_span)\n",
    "    n_spans[dn] = sum(spans) / len(spans)\n",
    "    n_chars[dn] = sum(map(len, sentences)) / len(sentences)\n",
    "    n_chars_seg[dn] = sum(seg_length) / len(seg_length)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint([n_spans, n_chars, n_chars_seg, n_samples])\n",
    "print(sum(n_spans.values())/1, sum(n_chars.values())/1, sum(n_chars_seg.values())/1)\n",
    "for dn in n_spans:\n",
    "    print('PHI:', n_spans[dn] * n_samples[dn] / entity_count[dn] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "legal-giant",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5598\n",
      "[{'ace04': 119.66784908094164,\n",
      "  'ace05': 98.44869160158926,\n",
      "  'conll03': 63.96182336182336,\n",
      "  'scierc': 136.5335840945728},\n",
      " {'ace04': 22.49742018703644,\n",
      "  'ace05': 19.943553911494725,\n",
      "  'conll03': 14.502635327635328,\n",
      "  'scierc': 24.401934443847395},\n",
      " {'ace04': 19.32838506522717,\n",
      "  'ace05': 16.320554445554446,\n",
      "  'conll03': 20.586655817738,\n",
      "  'scierc': 23.403130059363196},\n",
      " {'ace04': 6202, 'ace05': 7299, 'conll03': 14040, 'scierc': 1861}]\n",
      "104.65298703473177 20.336385967503475 19.909681346970704\n",
      "PHI: conll03 37.21866621270801\n",
      "PHI: ace04 32.42550891731219\n",
      "PHI: ace05 27.943368107302533\n",
      "PHI: scierc 44.38924615934262\n"
     ]
    }
   ],
   "source": [
    "n_samples = {}\n",
    "n_spans = {}\n",
    "n_chars = {}\n",
    "n_chars_seg = {}\n",
    "\n",
    "for dn in en_dataset_names:\n",
    "    file_path = f\"{data_dir}/{dn}/train.json\"\n",
    "    if dn == 'scierc':\n",
    "        n_entities = 0\n",
    "        sentences = []\n",
    "        for line in open(file_path, 'r'):\n",
    "            sentences.extend(json.loads(line)['sentences'])\n",
    "            for ner_case in json.loads(line)['ner']:\n",
    "                n_entities += len(ner_case)\n",
    "        print(n_entities)\n",
    "    else:\n",
    "        sentences = json.load(open(file_path, 'r'))['sentences']\n",
    "    \n",
    "    n_samples[dn] = len(sentences)\n",
    "    spans = []\n",
    "    seg_length = []\n",
    "    for sent in sentences:\n",
    "        _n_span = 0\n",
    "        streak = 0\n",
    "        for c in sent:\n",
    "            if c in ['.']:\n",
    "                if streak > 10:\n",
    "                    _n_span += (streak-10) * 10\n",
    "                else:\n",
    "                    _n_span += (1 + streak) * streak / 2 \n",
    "                seg_length.append(streak)\n",
    "                streak = 0\n",
    "            else:\n",
    "                streak += 1\n",
    "        else:\n",
    "            if streak > 10:\n",
    "                _n_span += (streak-10) * 10\n",
    "            else:\n",
    "                _n_span += (1 + streak) * streak / 2 \n",
    "            seg_length.append(streak)\n",
    "            streak = 0\n",
    "        spans.append(_n_span)\n",
    "    n_spans[dn] = sum(spans) / len(spans)\n",
    "    n_chars[dn] = sum(map(len, sentences)) / len(sentences)\n",
    "    n_chars_seg[dn] = sum(seg_length) / len(seg_length)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint([n_spans, n_chars, n_chars_seg, n_samples])\n",
    "print(sum(n_spans.values())/4, sum(n_chars.values())/4, sum(n_chars_seg.values())/4)\n",
    "for dn in n_spans:\n",
    "    print('PHI:', dn, n_spans[dn] * n_samples[dn] / entity_count[dn] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "public-ireland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span_counts 5.130883972997375 3.159165411426684\n",
      "sentence_length 2.561934562274675 2.561934562274675\n",
      "sentence_length on segments 1.329642750905446 0.7173527217209587\n"
     ]
    }
   ],
   "source": [
    "# n_spans\n",
    "print('span_counts', 9655005 / 1881743, 9655005 / 3056188.5)\n",
    "\n",
    "# n_chars\n",
    "print('sentence_length', 18831.5 / 7350.5, 18831.5 / 7350.5)\n",
    "\n",
    "# n_chars_seg\n",
    "print('sentence_length on segments', 14.282264102846437/10.74142967584387, 14.282264102846437/19.909681346970704)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('pure')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab478a5a1ebe8d3272f11e140b1127aeaad7a84ba8242059d9ceda920cc73b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
